name: Daily Boletines Scraping

on:
  schedule:
    # Ejecutar todos los días a las 8:00 AM UTC (9:00 AM España en invierno, 10:00 AM en verano)
    - cron: '0 8 * * *'
  workflow_dispatch: # Permite ejecutar manualmente desde GitHub

# Evitar ejecuciones concurrentes
concurrency:
  group: daily-scraping
  cancel-in-progress: false

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'  # Mismo que local
    
    - name: Configure SQLite environment
      run: |
        echo "Configurando entorno SQLite igual que local..."
        python -c "import sqlite3; print(f'SQLite version: {sqlite3.sqlite_version}')"
        python -c "import sys; print(f'Python version: {sys.version}')"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run daily scraping
      working-directory: tabs/buscador
      run: |
        echo "🔍 Estado BD ANTES del scraping:"
        ls -la data/boletines.db || echo "BD no existe"
        python -c "
        import sys, os
        sys.path.append('.')
        from database_simple import BoletinesDBSimple
        if os.path.exists('data/boletines.db'):
            db = BoletinesDBSimple('data/boletines.db')
            stats = db.obtener_estadisticas()
            print(f'📊 ANTES: {stats.get(\"total\", 0)} registros')
        else:
            print('📊 ANTES: BD no existe')
        " || true
        
        echo "🚀 Ejecutando scraping..."
        python scraper_diario.py
        
        echo "🔍 Estado BD DESPUÉS del scraping:"
        ls -la data/boletines.db
        python -c "
        import sys
        sys.path.append('.')
        from database_simple import BoletinesDBSimple
        db = BoletinesDBSimple('data/boletines.db')
        stats = db.obtener_estadisticas()
        print(f'📊 DESPUÉS: {stats.get(\"total\", 0)} registros')
        print(f'📋 Por fuente: {stats.get(\"por_fuente\", {})}')
        
        # Test de integridad
        import sqlite3
        conn = sqlite3.connect('data/boletines.db')
        cursor = conn.cursor()
        cursor.execute('PRAGMA integrity_check;')
        integrity = cursor.fetchone()[0]
        print(f'✅ Integridad: {integrity}')
        conn.close()
        "
    
    - name: Commit and push if changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        git add tabs/buscador/data/boletines.db
        git add tabs/buscador/logs/ || true
        if git diff --staged --quiet; then
          echo "No hay cambios para commitear"
        else
          git commit -m "🤖 Actualización diaria de boletines - $(date +'%Y-%m-%d %H:%M')"
          # Intentar push con reintentos en caso de conflictos
          for i in {1..3}; do
            if git push; then
              echo "Push exitoso en intento $i"
              break
            else
              echo "Push falló en intento $i, haciendo pull y reintentando..."
              git pull --rebase origin master || true
            fi
          done
        fi
    
    - name: Display scraping summary
      working-directory: tabs/buscador
      run: |
        python -c "
        from database_simple import BoletinesDBSimple
        db = BoletinesDBSimple('data/boletines.db')
        stats = db.obtener_estadisticas()
        print('📊 Estadísticas de la base de datos:')
        print(f'📚 Total boletines: {stats.get(\"total\", \"N/A\")}')
        print(f'📑 Por fuente: {stats.get(\"por_fuente\", {})}')
        print(f'📅 Rango: {stats.get(\"fecha_inicio\", \"N/A\")} - {stats.get(\"fecha_fin\", \"N/A\")}')
        print(f'🕒 Última actualización: {stats.get(\"ultimo_scraping\", \"N/A\")}')
        "